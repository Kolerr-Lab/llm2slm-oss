# Example environment configuration for LLM2SLM
# Copy this file to .env and fill in your actual values

# ────────────────────────────────────────────────────────────────────────────────
# General Configuration
# ────────────────────────────────────────────────────────────────────────────────

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LLM2SLM_LOG_LEVEL=INFO

# Output directory for converted models
LLM2SLM_OUTPUT_DIR=./output

# Cache directory for temporary files
LLM2SLM_CACHE_DIR=./cache

# ────────────────────────────────────────────────────────────────────────────────
# Server Configuration
# ────────────────────────────────────────────────────────────────────────────────

# Server host and port
LLM2SLM_SERVER_HOST=0.0.0.0
LLM2SLM_SERVER_PORT=8000

# Number of worker processes (for production)
LLM2SLM_WORKERS=1

# Enable debug mode (development only)
LLM2SLM_DEBUG=false

# ────────────────────────────────────────────────────────────────────────────────
# Provider API Keys
# ────────────────────────────────────────────────────────────────────────────────

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_TIMEOUT=30
OPENAI_MAX_RETRIES=3

# Hugging Face API Configuration (if supported)
# HUGGINGFACE_API_KEY=your_huggingface_token_here
# HUGGINGFACE_BASE_URL=https://api-inference.huggingface.co

# Anthropic API Configuration
ANTHROPIC_API_KEY=sk-ant-xxx

# Google API Configuration
GOOGLE_API_KEY=AIza-xxx

# LiquidAI API Configuration
LIQUID_API_KEY=liq-xxx

# ────────────────────────────────────────────────────────────────────────────────
# Conversion Settings
# ────────────────────────────────────────────────────────────────────────────────

# Default compression ratio (0.1 to 0.9)
LLM2SLM_COMPRESSION_RATIO=0.5

# Default quantization method (int8, int4, float16)
LLM2SLM_QUANTIZATION=int8

# Optimization level (1-3, higher = more optimization)
LLM2SLM_OPTIMIZATION_LEVEL=2

# Minimum accuracy to preserve (0.5 to 1.0)
LLM2SLM_PRESERVE_ACCURACY=0.95

# Default batch size for processing
LLM2SLM_BATCH_SIZE=32

# Maximum sequence length
LLM2SLM_MAX_LENGTH=2048

# ────────────────────────────────────────────────────────────────────────────────
# Database Configuration (if using external database)
# ────────────────────────────────────────────────────────────────────────────────

# DATABASE_URL=postgresql://user:password@localhost:5432/llm2slm
# REDIS_URL=redis://localhost:6379/0

# ────────────────────────────────────────────────────────────────────────────────
# Security Settings
# ────────────────────────────────────────────────────────────────────────────────

# Secret key for JWT tokens (generate a secure random string)
# SECRET_KEY=your_secret_key_here

# Allowed CORS origins (comma-separated)
CORS_ORIGINS=*

# API rate limiting
# RATE_LIMIT_PER_MINUTE=60

# ────────────────────────────────────────────────────────────────────────────────
# Hardware Configuration
# ────────────────────────────────────────────────────────────────────────────────

# CUDA device ID (if using GPU)
# CUDA_VISIBLE_DEVICES=0

# Enable GPU acceleration
# USE_GPU=true

# Memory limit per process (in GB)
# MEMORY_LIMIT=8

# ────────────────────────────────────────────────────────────────────────────────
# Development Settings
# ────────────────────────────────────────────────────────────────────────────────

# Enable development mode
# DEVELOPMENT=true

# Enable hot reload
# HOT_RELOAD=true

# Enable profiling
# ENABLE_PROFILING=false

# ────────────────────────────────────────────────────────────────────────────────
# Monitoring and Observability
# ────────────────────────────────────────────────────────────────────────────────

# Enable metrics collection
# ENABLE_METRICS=true

# Metrics port
# METRICS_PORT=9090

# Enable distributed tracing
# ENABLE_TRACING=false

# Jaeger endpoint (if tracing enabled)
# JAEGER_ENDPOINT=http://localhost:14268/api/traces
